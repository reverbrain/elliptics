###################################
# elliptics-core

## log file
# set to 'syslog' without inverted commas if you want elliptics to log through syslog
# if you want write log to the file, create appropriate directory first
log = /dev/stderr

## log level. It doesn't corresponds to syslog levels. When logging to syslog 
# almost all messages will be written with INFO level.
# DNET_LOG_DATA		= 0
# DNET_LOG_ERROR	= 1
# DNET_LOG_INFO		= 2
# DNET_LOG_NOTICE	= 3
# DNET_LOG_DEBUG	= 4
log_level = 3

## specifies whether to join storage network. Server nodes should use 1, 
# client nodes don't use this parameter or set it to 0.
join = 1

## config flags
# bits start from 0, 0 is unused (its actuall above join flag)
# bit 1 (flags=1) - do not request remote route table
# bit 2 (flags=4) - mix states before read operations according to state's weights
# bit 3 (flags=8) - do not checksum data on upload and check it during data read
# bit 4 (flags=16) - do not update metadata at all
# bit 5 (flags=32) - randomize states for read requests
# bits can be set in any variations, but in case of bits 2 and 5 set both, 2 will be used.
flags = 4

## node will join nodes in this group
group = 2

## list of remote nodes to connect
#
# `address:port:family` where family is either 2 (AF_INET) or 10 (AF_INET6)
# address can be host name or IP
#
# Multicast doesn't used this time.
# It is possible to autodiscover remote clusters via multicast.
# If you put `autodiscovery:address:port:family` where `address:port:family` is valid multicast address,
# elliptics will broadcast information about itself and remote nodes with the same auth cookie will
# receive this information and connect to given node. Multicast TTL equals to 3.
#
remote = 1.2.3.4:1025:2 2.3.4.5:2345:2 autodiscovery:224.0.0.5:1025:2

## multiple interface support
# Elliptics server listens on port specified in this config option (format: addr:port:family-route_group)
# If ports differ, the last one will be used.
#
# Elliptics server will accept connections from any address, but it has to understand which addresses of
# the other connected joined servers it has to send to newly accepted client. Thus we 'join' multiple
# addresses on every node into 'logical route tables' which are indexed by the last number in addr
# config option. Thus, format becomes: local_address:port:family-route_group
#
# Addresses in the same route group on different servers will be put into the same route tables,
# thus when client in example below connects to localhost, it will receive (and connect to) addresses
# from the logical route group 0 (whatever addresses will be put into that route group on other servers).
#
# Let's suppose we have 3 connected servers with the following addresses
# srv0: 10.0.0.17:1025:2-0 192.168.0.17:1025:2-1 20.20.20.17:1025:2-2
# srv1: 10.0.0.34:1025:2-0 192.168.0.34:1025:2-1 20.20.20.34:1025:2-2
# srv2: 15.14.13.12:1025:2-0 99.99.99.99:1025:2-1 111.111.111.111:1025:2-2
#
# When client connects to srv1 to IP address 192.168.0.34:1025, it will receive (and connect to)
# following route table:
# 192.168.0.17:1025 192.168.0.34:1025 99.99.99.99:1025
#
# Because above addresses are in the same logical route group and client
# connected to one of the addresses in that logical route group.
#
# `addr` is a list of interfaces that server binds
addr = localhost:1025:2-0 10.10.0.1:1025:2-1

## wait timeout specifies number of seconds to wait for command completion
wait_timeout = 60

## this timeout specifies number of seconds to wait before killing
# unacked transaction. This parameter is used also for some service operations such as
# update the routing server, check network connections an so on.
check_timeout = 60

## number of IO threads in processing pool. Typically, value of this parameter should be comparable
# with the number of hardware processing cores.
io_thread_num = 16

## number of IO threads in processing pool dedicated to nonblocking operations
# they are invoked from recursive commands like DNET_CMD_EXEC, when script
# tries to read/write some data using the same id/key as in original exec command
# Typically, value of this parameter should be comparable with the number of hardware processing cores.
nonblocking_io_thread_num = 16

## number of threads in network processing pool
net_thread_num = 16

## specifies history environment directory
# it will host file with generated IDs
# and server-side execution scripts
# should be created manually before use
history = /tmp/history

## specifies whether to go into background
daemon = 0

## authentication cookie
# if this string (32 bytes long max) does not match to server nodes of some group,
# new node can not join to that group and serve IO
auth_cookie = qwerty

## Background jobs (replica checks and recovery) IO priorities
# ionice for background operations (disk scheduler should support it)
# class - number from 0 to 3
# 0 - default class
# 1 - realtime class
# 2 - best-effort class
# 3 - idle class
bg_ionice_class = 3
# prio - number from 0 to 7, sets priority inside class
bg_ionice_prio = 0

## IP priorities
# man 7 socket for IP_PRIORITY
# server_net_prio is set for all joined (server) connections
# client_net_prio is set for other connection
# is only turned on when non zero
server_net_prio = 1
client_net_prio = 6

## In-memory cache support
# This is maximum cache size. Cache is managed by LRU algorithm
# Using different IO flags in read/write/remove commands one can use it
# as cache for data, stored on disk (in configured backend),
# or as plain distributed in-memory cache
cache_size = 102400

###################################
#SRW - server-side scripting

## srw worker config
# Elliptics uses cocaine engine (https://github.com/organizations/cocaine) for its server-side workers
# srw_config should point to its configuration file, example config lives in tree in example/library_config.json file
# If you use this parameter, cocaine should be installed and configured with a confidence, or elliptics will not start.
# srw_config = /opt/elliptics/library_config.json

###################################
# backend
# anything below this line will be processed
# by backend's parser and will not be able to
# change global configuration
# backend can be 'filesystem' or 'blob'

backend = filesystem

## Number of bits (from the beginning of the object ID) used
# for directory, which hosts given object
directory_bit_number = 8

## Root directory for data objects. Should be created manually before use.
root = /tmp/root

## zero here means 'sync on every write'
# positive number means file writes are never synced
# and metadata is synced every `sync` seconds
sync = 0


#backend = blob

## zero here means 'sync on every write'
# positive number means data and metadata updates
# are synced every `sync` seconds
#sync = 0

## eblob objects prefix. System will append .NNN and .NNN.index to new blobs. Path to blobs should be created manually before use.
# If prefix is `/tmp/blob/data`, path `/tmp/blob` should be created.
#data = /tmp/blob/data

## Align all writes to this boundary
#data_block_size = 1024

## blob processing flags (bits start from 0)
# bit 0 - if set, eblob reserves 10% of total space or size of the blob (which is bigger)
# 		By default it is turned off and eblob only reserves size of the blob
# 		This is useful (required) to be able to run defragmentation
# bit 1 - overwrite commits write - when set, every overwrite operation will commit its size
# 		To turn overwrite-commits mode you must turn on overwrite mode too, i.e. set bit 2 (blob_flags=6 in config file)
# 		as final, otherwise we will just overwrite
# 		Without this bit set it is equivalent to overwrite parts of the file
# 		When this bit is set, it is like overwriting data and truncating file to given offset + size
# bit 2 - turn on overwrite mode - data can be overwritten in place instead
# 		of appending it at the end. This mode is turned on for metadata
# 		writes (column 1), this bit enables it for all other writes too
# bit 3 - do not append checksum footer - this saves 72 bytes per written record.
# 		This also disables checksum.
# bit 4 - do not check whether system has enough space for the new blob
# bit 5 - reserved for internal use, do not set
# bit 6 - use second hashing layer - greatly reduces memory usage for in-memory eblob index (costs some IOPS)
# 		Likely recommended for everyday use
# bit 7 - auto sort. When set, eblob will honour defrag* params below and will also perform data sorting as well as
# 		defragmentation and index sorting. If you do not want to preiodically defragment your blob (this
# 		option also turns on defragmentation on startup and when blob reaches its maximum size of maximum number
# 		of records), it is recommended to disable this option and manually run 'dnet_ioclient ... -d' command when needed.
# 		Manual check will defragment and sort data. It honours `defrag_percentage` config option.
#blob_flags = 1

## Number of threads used to populate data into RAM at startup. In most cases is not used.
#iterate_thread_num = 1

## Maximum blob size. New file will be opened after current one
# grows beyond `blob_size` limit
# Supports K, M and G modifiers
#blob_size = 10G

## Maximum number of records in blob.
# When number of records reaches this level,
# blob is closed and sorted index is generated.
# Its meaning is similar to above `blob_size`,
# except that it operates on records and not bytes.
# Both parameters `blob_size` and `records_in_blob` can be used together or separately
#records_in_blob = 10000000

## Timeout for defragmentation process to start
# In every time slot eblob will defragment one blob only,
# since system reserves enough space for only one blob.
# After next timeout old (already defragmented into copy)
# blob will be closed (this will actually free space) and
# next one will be defragmented.
#
# Defragmentation operation is rather costly (even if nothing
# is going to be copied, defragmentation still checks every index
# to determine number of removed keys)
# It is recommended to set it to hours (it is in seconds) or more
# Default: -1 or none
#defrag_timeout = 3600

## Percentage of removed entries (compared to number of all keys in blob)
# needed to start defragmentation. If number of removed keys is less than
# (removed + not removed) * $defrag_percentage / 100 then defragmentation
# process will skip given blob
#defrag_percentage = 25

## Maximum size whole eblob can occupy on disk
# This size will account for all columns (data-XXX.* files) and appropriate indexes
# Basically, this is the maximum size eblob data directory can occupy on disk
#blob_size_limit = 10G

## Bloom filter parameters
# index_block_size - number of records from index file, which are hashed into one bloom filter
# eblob splits all records from sorted index file into chunks, each chunk has start and finish
# keys only and bloom filter which says whether requested entry can be found in given chunk
# index_block_bloom_length - number of bits per chunk, it should be at least as twice as number
# of records in chunk
#
# Default values:
# index_block_size = 40
# index_block_bloom_length = 128 * 40
